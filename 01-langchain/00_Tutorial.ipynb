{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837defd9",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction to LangChain and Its Usage in Generative AI\n",
    "\n",
    "### 1. Theory\n",
    "- **LangChain** is an open-source framework designed to facilitate building applications powered by **Large Language Models (LLMs)**.\n",
    "- It provides abstractions to connect LLMs with **data sources, APIs, and other tools** for building advanced AI applications.\n",
    "- Core philosophy: \"Language models are more useful when connected to external data and processes.\"\n",
    "- **Key Components of LangChain**:\n",
    "  1. **LLM wrappers** – Interfaces for interacting with models like OpenAI, Hugging Face, etc.\n",
    "  2. **Prompt templates** – Structured templates to guide LLM outputs.\n",
    "  3. **Chains** – Sequential or branched workflows combining multiple LLM calls.\n",
    "  4. **Agents** – LLMs making decisions and interacting with tools dynamically.\n",
    "  5. **Memory** – Enables context persistence across interactions.\n",
    "  6. **Data connectors** – Integrates databases, APIs, and documents.\n",
    "\n",
    "### 2. Example\n",
    "```python\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the topic {topic} in simple words.\"\n",
    ")\n",
    "\n",
    "# Build a chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run(\"LangChain\")\n",
    "print(response)\n",
    "````\n",
    "\n",
    "### 3. Use Cases\n",
    "\n",
    "| Use Case                      | Description                                                        |\n",
    "| ----------------------------- | ------------------------------------------------------------------ |\n",
    "| Chatbots & Virtual Assistants | Build intelligent conversational agents with memory and reasoning  |\n",
    "| Document Q\\&A                 | Answer questions by integrating LLMs with company documents        |\n",
    "| Automation                    | Use agents to call APIs, scrape data, or execute tasks dynamically |\n",
    "| Personalized Education        | Generate adaptive learning content for students                    |\n",
    "| Code Generation               | Assist developers with automated code suggestions and explanations |\n",
    "\n",
    "### 4. Architecture / Diagram\n",
    "\n",
    "```python\n",
    "from IPython.display import Image\n",
    "# Image link showing LangChain architecture\n",
    "Image(\"https://raw.githubusercontent.com/hwchase17/langchain-docs/master/docs/_static/langchain_architecture.png\")\n",
    "```\n",
    "\n",
    "**Description:**\n",
    "\n",
    "* User interacts with **LLM** through chains or agents.\n",
    "* Chains define workflows; agents make decisions using tools.\n",
    "* Memory stores conversation or state information.\n",
    "* Data connectors allow LLM to access external knowledge.\n",
    "\n",
    "### 5. Interview Q\\&A\n",
    "\n",
    "| Question                                                | Answer                                                                                                                              |\n",
    "| ------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| What is LangChain?                                      | A framework to build AI applications by connecting LLMs with data, APIs, and workflows.                                             |\n",
    "| What are the core components of LangChain?              | LLM wrappers, Prompt Templates, Chains, Agents, Memory, Data Connectors.                                                            |\n",
    "| How does LangChain differ from directly using LLM APIs? | LangChain provides structured workflows, memory, decision-making, and integration capabilities, making LLMs more application-ready. |\n",
    "| Can LangChain work with multiple LLM providers?         | Yes, it supports OpenAI, Hugging Face, and other APIs via standardized wrappers.                                                    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8cf3c4",
   "metadata": {},
   "source": [
    "\n",
    "## Data Ingestion, Splitting, and Embedding in LangChain\n",
    "\n",
    "### 1. Theory\n",
    "LangChain provides modules to **ingest, process, and structure data** so that LLMs can efficiently query it. Key steps include:\n",
    "\n",
    "1. **Data Ingestion / Document Loading**\n",
    "   - Load data from various sources like PDFs, Word files, CSVs, web pages, and databases.\n",
    "   - Document loaders standardize inputs for downstream processing.\n",
    "   - Example loaders: `PyPDFLoader`, `CSVLoader`, `WebBaseLoader`.\n",
    "\n",
    "2. **Data Splitting / Chunking**\n",
    "   - Large documents are split into smaller **chunks** to avoid token limits of LLMs.\n",
    "   - Strategies:\n",
    "     - **Character splitting**: Split by characters or sentences.\n",
    "     - **Recursive splitting**: Preserve semantic meaning across chunks.\n",
    "   - Helps improve retrieval quality and response accuracy.\n",
    "\n",
    "3. **Embedding**\n",
    "   - Convert text chunks into **vector representations**.\n",
    "   - Vectors capture semantic meaning, enabling similarity searches.\n",
    "   - Popular embedding models: OpenAI `text-embedding-3-small` or `text-embedding-3-large`, Hugging Face models.\n",
    "   \n",
    "4. **Vector Stores**\n",
    "   - Store embeddings for fast similarity-based retrieval.\n",
    "   - Examples: FAISS, Pinecone, Weaviate.\n",
    "   - Used in **Retrieval-Augmented Generation (RAG)** pipelines.\n",
    "\n",
    "5. **Retrieval**\n",
    "   - Retrieve relevant chunks based on query embeddings.\n",
    "   - Feed retrieved content into LLMs to produce informed responses.\n",
    "\n",
    "### 2. Example\n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Step 1: Load document\n",
    "loader = PyPDFLoader(\"sample.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split document into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Create embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Step 4: Retrieve similar chunks\n",
    "query = \"Explain LangChain data workflow\"\n",
    "results = vectorstore.similarity_search(query)\n",
    "print(results[0].page_content)\n",
    "````\n",
    "\n",
    "### 3. Use Cases\n",
    "\n",
    "| Step             | Use Case                                                         |\n",
    "| ---------------- | ---------------------------------------------------------------- |\n",
    "| Document Loaders | Load PDFs, web pages, or company docs into LLM pipelines         |\n",
    "| Text Splitters   | Ensure chunks fit token limits and maintain semantic meaning     |\n",
    "| Embeddings       | Transform text into vectors for semantic search                  |\n",
    "| Vector Stores    | Enable fast retrieval for question answering or RAG applications |\n",
    "| Retrieval        | Feed relevant context into LLMs for accurate responses           |\n",
    "\n",
    "### 4. Architecture / Diagram\n",
    "\n",
    "```python\n",
    "from IPython.display import Image\n",
    "# LangChain Data Workflow\n",
    "Image(\"https://raw.githubusercontent.com/hwchase17/langchain-docs/master/docs/_static/langchain_data_workflow.png\")\n",
    "```\n",
    "\n",
    "**Description:**\n",
    "\n",
    "* Data is ingested → split into chunks → embedded into vectors → stored in a vector store → retrieved for LLM queries.\n",
    "* Ensures scalable, accurate, and token-efficient usage of LLMs.\n",
    "\n",
    "### 5. Interview Q\\&A\n",
    "\n",
    "| Question                                            | Answer                                                                                                |\n",
    "| --------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |\n",
    "| What is the purpose of text splitting in LangChain? | To divide large documents into manageable chunks that fit LLM token limits while preserving context.  |\n",
    "| Why are embeddings used?                            | To convert text into vector representations that capture semantic meaning for similarity search.      |\n",
    "| Name some vector stores compatible with LangChain.  | FAISS, Pinecone, Weaviate, Milvus.                                                                    |\n",
    "| What is Retrieval-Augmented Generation (RAG)?       | A pipeline where retrieved relevant data is fed into an LLM to generate informed, accurate responses. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219123a",
   "metadata": {},
   "source": [
    "## Data Ingestion in LangChain\n",
    "\n",
    "### 1. Theory\n",
    "Data ingestion is the **first step in building knowledge-augmented applications** using LangChain.  \n",
    "It refers to the process of **loading unstructured or structured data** from multiple sources into a standardized format that can be processed further (e.g., splitting, embedding, retrieval).\n",
    "\n",
    "#### Key Points:\n",
    "- **Goal**: Convert external data sources into a format usable by LLMs.\n",
    "- **Supported Sources**: PDFs, Word docs, CSVs, Excel, HTML, SQL/NoSQL databases, APIs, web scraping.\n",
    "- **LangChain Module**: `DocumentLoaders`\n",
    "  - Each loader parses the data source into a **Document object** (with text + metadata).\n",
    "\n",
    "#### Popular Document Loaders:\n",
    "| Loader | Purpose |\n",
    "|--------|---------|\n",
    "| `PyPDFLoader` | Load PDF documents |\n",
    "| `UnstructuredFileLoader` | Handle text, Word, PowerPoint |\n",
    "| `CSVLoader` | Load CSV files as rows of documents |\n",
    "| `WebBaseLoader` | Scrape and load data from a webpage |\n",
    "| `DirectoryLoader` | Bulk load files from a folder |\n",
    "| `SQLDatabaseLoader` | Ingest data from databases |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Example\n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader, CSVLoader, WebBaseLoader\n",
    "\n",
    "# Example 1: PDF Loader\n",
    "pdf_loader = PyPDFLoader(\"company_report.pdf\")\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# Example 2: CSV Loader\n",
    "csv_loader = CSVLoader(\"employee_feedback.csv\")\n",
    "csv_docs = csv_loader.load()\n",
    "\n",
    "# Example 3: Web Loader\n",
    "web_loader = WebBaseLoader(\"https://example.com/blog\")\n",
    "web_docs = web_loader.load()\n",
    "\n",
    "print(\"PDF Docs:\", pdf_docs[0])\n",
    "print(\"CSV Docs:\", csv_docs[0])\n",
    "print(\"Web Docs:\", web_docs[0])\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Use Cases\n",
    "\n",
    "| Use Case                | Description                                               |\n",
    "| ----------------------- | --------------------------------------------------------- |\n",
    "| Knowledge Base Creation | Load company documents for internal Q\\&A                  |\n",
    "| Chat with PDFs          | Extract information from research papers, reports         |\n",
    "| Customer Support        | Ingest FAQs and support tickets for AI assistants         |\n",
    "| Data Integration        | Pull structured/unstructured data into a unified pipeline |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Architecture / Diagram\n",
    "\n",
    "```python\n",
    "from IPython.display import Image\n",
    "# Data ingestion flow\n",
    "Image(\"https://raw.githubusercontent.com/hwchase17/langchain-docs/master/docs/_static/document_loader.png\")\n",
    "```\n",
    "\n",
    "**Description:**\n",
    "\n",
    "* **Raw Data Source** → **Document Loader** → **Document Objects (text + metadata)** → passed into splitters/embeddings/vector stores.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Interview Q\\&A\n",
    "\n",
    "| Question                                          | Answer                                                                                                                   |\n",
    "| ------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
    "| What is Data Ingestion in LangChain?              | The process of loading external data (PDFs, CSVs, web, databases) into standardized Document objects for LLM processing. |\n",
    "| Which LangChain module is used for ingestion?     | `DocumentLoaders`.                                                                                                       |\n",
    "| How does LangChain handle different file formats? | Through specialized loaders like `PyPDFLoader`, `CSVLoader`, `WebBaseLoader`.                                            |\n",
    "| Why is ingestion important before embeddings?     | LLMs can’t directly process large or raw documents; ingestion converts them into usable structured text chunks.          |\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4315a9f3",
   "metadata": {},
   "source": [
    "\n",
    "## Text Splitters in LangChain – Techniques, Tuning, and Implementation\n",
    "\n",
    "### 1. Theory\n",
    "Text splitting is the foundational pre-processing capability that **segments raw content into retrievable, token-budget–aware chunks** while preserving semantic fidelity. Effective splitting maximizes downstream **RAG** precision/recall, reduces hallucinations, and optimizes latency/cost profiles.\n",
    "\n",
    "**Objectives**\n",
    "- Fit context windows (token limits) without truncation.\n",
    "- Preserve local coherence (minimize “thought-splits”).\n",
    "- Enrich chunks with metadata (source, page, headers) for performant retrieval filters.\n",
    "\n",
    "**Core Concepts**\n",
    "- **Chunk Size (`chunk_size`)**: Target characters/tokens per chunk.\n",
    "- **Chunk Overlap (`chunk_overlap`)**: Sliding-window overlap to retain context across boundaries.\n",
    "- **Granularity**: Character → sentence → paragraph → section (e.g., markdown headers).\n",
    "- **Semantics-Aware**: Prefer boundaries at **natural discourse units** (sentences/headers/code blocks).\n",
    "- **Domain-Specific Splitters**: Markdown, HTML, code, LaTeX, Python, notebook cells, etc.\n",
    "\n",
    "**Why Overlap?**  \n",
    "To mitigate boundary effects—e.g., an answer fragment at the end of chunk *i* is still visible at the start of chunk *i+1*.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Techniques & Splitter Types\n",
    "\n",
    "| Splitter | What it does | When to use | Pros | Cons |\n",
    "|---|---|---|---|---|\n",
    "| **RecursiveCharacterTextSplitter** | Attempts larger boundaries first (e.g., paragraphs → sentences → words) then falls back | General-purpose RAG on prose | High-quality chunks, robust | Slightly higher CPU than naive splits |\n",
    "| **CharacterTextSplitter** | Fixed-size character windows with optional separators | Logs, transcripts, very large corpora | Fast, predictable | Can split mid-sentence; lower semantic fidelity |\n",
    "| **TokenTextSplitter** | Splits by model tokenizer tokens (e.g., tiktoken) | Strict token-budget scenarios | Aligns to LLM limits | Requires tokenizer, still not semantics-aware |\n",
    "| **MarkdownHeaderTextSplitter** | Splits by `#`, `##`, `###` headers; attaches header path as metadata | Docs, READMEs, wikis | Great topical cohesion | Header quality dependent |\n",
    "| **HTML/BS4 Splitter** | Splits by DOM structure (e.g., `<h1>`, `<p>`, `<li>`) | Web pages, knowledge bases | Structure-aware | Requires clean HTML |\n",
    "| **CodeTextSplitter** | Language-aware code blocks (functions/classes) | Code RAG, snippet search | Preserves syntactic units | Language heuristics vary |\n",
    "| **Notebook/Doc-Specific** (e.g., LaTeX, Jupyter) | Domain-aware cells/sections | Research, notebooks | Strong semantic boundaries | Niche, setup-dependent |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Practical Heuristics (Tuning Playbook)\n",
    "\n",
    "| Content Type | `chunk_size` | `chunk_overlap` | Notes |\n",
    "|---|---:|---:|---|\n",
    "| Prose (knowledge articles) | 800–1200 chars (or 300–500 tokens) | 10–15% | Start with RecursiveCharacter |\n",
    "| PDFs (scanned/structured) | 800–1000 | 15–20% | Normalize whitespace; keep page metadata |\n",
    "| Markdown docs | 1000–1500 | 10–15% | Use MarkdownHeader splitter + recursive |\n",
    "| Web pages (HTML) | 800–1200 | 10–15% | Strip nav/boilerplate, retain headings |\n",
    "| Code | 200–400 | 15–25% | Use CodeTextSplitter to avoid breaking symbols |\n",
    "| Meeting transcripts | 800–1200 | 10–20% | Consider sentence/turn-based splitting |\n",
    "| Legal/Compliance | 1000–1500 | 10–20% | Preserve section/article identifiers |\n",
    "\n",
    "> **Guardrails**: If retrieval returns “near-duplicates,” overlap is too high. If answers lack context, increase overlap or reduce chunk size.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Code Examples (Legacy & Current Imports)\n",
    "\n",
    "> LangChain introduced namespaced packages. Below are **both** patterns to align with your environment.\n",
    "\n",
    "#### 4.1 Recommended (Current) Imports\n",
    "```python\n",
    "# If you're on LangChain 0.1+/0.2+ (modularized)\n",
    "# pip install langchain-text-splitters tiktoken\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    ")\n",
    "import tiktoken\n",
    "\n",
    "# Example: Recursive split for general prose\n",
    "text = \"\"\"# Product Overview\n",
    "LangChain orchestrates LLM apps...\n",
    "## Features\n",
    "- Chains\n",
    "- Agents\n",
    "- Memory\n",
    "## Use Cases\n",
    "RAG, chatbots, automation...\n",
    "\"\"\"\n",
    "\n",
    "# 1) Markdown headers first -> then recursive inside sections\n",
    "headers_to_split_on = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]\n",
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_docs = md_splitter.split_text(text)\n",
    "\n",
    "# Apply a recursive splitter to each section document\n",
    "rec_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "chunks = []\n",
    "for d in md_docs:\n",
    "    chunks.extend(rec_splitter.split_text(d.page_content))\n",
    "\n",
    "len(chunks), chunks[:2]\n",
    "````\n",
    "\n",
    "#### 4.2 Legacy Imports (Monolithic)\n",
    "\n",
    "```python\n",
    "# Older versions: pip install langchain==0.0.3xx (example)\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter\n",
    ")\n",
    "import tiktoken\n",
    "\n",
    "# Token-based split (strict token budget)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "token_splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=60\n",
    ")\n",
    "token_chunks = token_splitter.split_text(\"Your long text here...\")\n",
    "token_chunks[:3]\n",
    "```\n",
    "\n",
    "#### 4.3 Code-Aware Split (Python)\n",
    "\n",
    "```python\n",
    "# pip install langchain-text-splitters\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "code = \"\"\"\n",
    "def add(a, b):\n",
    "    # Adds two numbers\n",
    "    return a + b\n",
    "\n",
    "class Calculator:\n",
    "    def mul(self, x, y):\n",
    "        return x * y\n",
    "\"\"\"\n",
    "\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "code_chunks = code_splitter.split_text(code)\n",
    "code_chunks\n",
    "```\n",
    "\n",
    "#### 4.4 HTML-Aware Split (DOM Structure)\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "html = \"\"\"<html><body><h1>Intro</h1><p>LangChain...</p><h2>RAG</h2><p>Details...</p></body></html>\"\"\"\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "text = \" \".join([t.get_text(\" \", strip=True) for t in soup.find_all([\"h1\",\"h2\",\"h3\",\"p\",\"li\"])])\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
    "chunks = splitter.split_text(text)\n",
    "chunks[:2]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. End-to-End Pattern (Splitter → Embeddings → Retrieval)\n",
    "\n",
    "```python\n",
    "# Split (recursive) -> embed -> store (FAISS) -> query\n",
    "# pip install langchain-text-splitters faiss-cpu sentence-transformers\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "corpus = \"Your organization’s knowledge base text ...\" * 20\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = splitter.create_documents([corpus])  # attaches source metadata indices\n",
    "\n",
    "embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vs = FAISS.from_documents(docs, embed)\n",
    "\n",
    "query = \"What is our RAG architecture?\"\n",
    "hits = vs.similarity_search(query, k=3)\n",
    "[ (h.metadata, h.page_content[:160]) for h in hits ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Architecture / Diagram (Inline SVG Generator)\n",
    "\n",
    "```python\n",
    "# Run this cell in Jupyter to render an SVG architecture diagram for splitting\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "svg = \"\"\"\n",
    "<svg width=\"880\" height=\"240\" xmlns=\"http://www.w3.org/2000/svg\" style=\"font-family:Inter,Arial,sans-serif\">\n",
    "  <rect x=\"10\" y=\"30\" width=\"200\" height=\"60\" rx=\"12\" ry=\"12\" fill=\"#eef2ff\" stroke=\"#6366f1\"/>\n",
    "  <text x=\"110\" y=\"65\" text-anchor=\"middle\" font-size=\"14\">Raw Content</text>\n",
    "  <text x=\"110\" y=\"83\" text-anchor=\"middle\" font-size=\"12\">(PDF/HTML/MD/Code)</text>\n",
    "\n",
    "  <polygon points=\"220,60 250,45 250,75\" fill=\"#6366f1\"/>\n",
    "\n",
    "  <rect x=\"250\" y=\"20\" width=\"240\" height=\"120\" rx=\"12\" ry=\"12\" fill=\"#ecfeff\" stroke=\"#06b6d4\"/>\n",
    "  <text x=\"370\" y=\"55\" text-anchor=\"middle\" font-size=\"14\">Text Splitters</text>\n",
    "  <text x=\"370\" y=\"75\" text-anchor=\"middle\" font-size=\"12\">Recursive / Token / Markdown / Code</text>\n",
    "  <text x=\"370\" y=\"95\" text-anchor=\"middle\" font-size=\"12\">chunk_size • chunk_overlap</text>\n",
    "\n",
    "  <polygon points=\"490,80 520,65 520,95\" fill=\"#06b6d4\"/>\n",
    "\n",
    "  <rect x=\"520\" y=\"30\" width=\"160\" height=\"60\" rx=\"12\" ry=\"12\" fill=\"#ecfccb\" stroke=\"#84cc16\"/>\n",
    "  <text x=\"600\" y=\"65\" text-anchor=\"middle\" font-size=\"14\">Chunks + Metadata</text>\n",
    "\n",
    "  <polygon points=\"680,60 710,45 710,75\" fill=\"#84cc16\"/>\n",
    "\n",
    "  <rect x=\"710\" y=\"30\" width=\"160\" height=\"60\" rx=\"12\" ry=\"12\" fill=\"#ffe4e6\" stroke=\"#f43f5e\"/>\n",
    "  <text x=\"790\" y=\"55\" text-anchor=\"middle\" font-size=\"14\">Embeddings</text>\n",
    "  <text x=\"790\" y=\"75\" text-anchor=\"middle\" font-size=\"12\">→ Vector Store</text>\n",
    "</svg>\n",
    "\"\"\"\n",
    "display(SVG(svg))\n",
    "```\n",
    "\n",
    "> The pipeline demonstrates **Raw Content → Splitters (strategy + params) → Chunks with metadata → Embeddings/Vector Store**. Splitting quality is a first-order driver of RAG accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Operational Best Practices & Anti-Patterns\n",
    "\n",
    "**Best Practices**\n",
    "\n",
    "* Normalize whitespace; remove boilerplate (navbars/footers).\n",
    "* Attach **source/page/section** metadata for traceability.\n",
    "* Use **domain-aware splitters** (Markdown, Code) where applicable.\n",
    "* Validate with retrieval probes (top-k inspection) before scaling.\n",
    "\n",
    "**Anti-Patterns**\n",
    "\n",
    "* Oversized chunks that exceed tokenizer limits (truncation).\n",
    "* Zero overlap on discourse-heavy text.\n",
    "* Blind character splitting on code or legal documents.\n",
    "* Skipping QA of retrieved contexts before prompting.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Use Cases\n",
    "\n",
    "| Scenario                    | Splitter Strategy               | Outcome                                           |\n",
    "| --------------------------- | ------------------------------- | ------------------------------------------------- |\n",
    "| Engineering wiki (Markdown) | MarkdownHeader → Recursive      | Topical, navigable chunks; better grounding       |\n",
    "| API docs (HTML)             | DOM extraction → Recursive      | High-precision endpoint Q\\&A                      |\n",
    "| Code RAG                    | CodeTextSplitter (language)     | Cohesive function/class chunks for repair/explain |\n",
    "| Contracts/Policies          | Recursive with section tags     | Traceable citations with section granularity      |\n",
    "| Meeting notes               | Sentence/turn-based → Recursive | Reduced context loss across speaker turns         |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Interview Q\\&A\n",
    "\n",
    "| Question                                            | Answer                                                                                                                       |\n",
    "| --------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Why is text splitting critical in RAG?              | It shapes retrieval fidelity and token efficiency, directly impacting answer accuracy and cost/latency envelopes.            |\n",
    "| Recursive vs Character splitters?                   | Recursive respects semantic boundaries via prioritized separators; Character is fast but can fragment thoughts.              |\n",
    "| How do you choose `chunk_size` and `chunk_overlap`? | Align to model context + content type; start 800–1200 chars with 10–20% overlap, then iterate via retrieval quality metrics. |\n",
    "| When prefer token-based splitting?                  | When you must **guarantee** token budgets (e.g., tight context windows or cost-constrained workloads).                       |\n",
    "| How to handle markdown or code?                     | Use domain-aware splitters (MarkdownHeader, CodeTextSplitter) to preserve structural/semantic units and metadata.            |\n",
    "| What signals indicate poor splitting?               | Redundant near-duplicates retrieved; missing context near boundaries; low exact citation rates.                              |\n",
    "| How to validate a splitter config?                  | A/B retrieval on queries, manual top-k inspection, and answer-grounding audits with source-level citations.                  |\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b095d9ee",
   "metadata": {},
   "source": [
    "## OpenAI Embeddings in LangChain\n",
    "\n",
    "### 1. Theory\n",
    "Embeddings are **dense vector representations of text** that capture semantic meaning. Instead of comparing raw words, embeddings allow us to compute **similarity in a vector space**.  \n",
    "\n",
    "OpenAI provides **state-of-the-art embedding models** that integrate seamlessly into LangChain.\n",
    "\n",
    "#### Key Points:\n",
    "- **Purpose**: Convert text into numerical vectors for search, clustering, retrieval, and RAG pipelines.\n",
    "- **Models**:\n",
    "  - `text-embedding-3-small` (cheap, efficient, 1536-dim vector).\n",
    "  - `text-embedding-3-large` (higher quality, 3072-dim vector).\n",
    "- **Distance Metrics**:\n",
    "  - Cosine similarity (most common).\n",
    "  - Dot product.\n",
    "  - Euclidean distance.\n",
    "- **LangChain Module**: `OpenAIEmbeddings`\n",
    "  - Converts raw text chunks into embeddings.\n",
    "  - Works with vector stores like FAISS, Pinecone, Weaviate, Chroma.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Example\n",
    "```python\n",
    "# Install required packages\n",
    "# pip install openai langchain-community faiss-cpu\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Step 1: Initialize embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Step 2: Example documents\n",
    "documents = [\"LangChain enables LLM workflows\",\n",
    "             \"OpenAI embeddings create vector representations\",\n",
    "             \"RAG improves knowledge-augmented answers\"]\n",
    "\n",
    "# Step 3: Create vector store\n",
    "vectorstore = FAISS.from_texts(documents, embeddings)\n",
    "\n",
    "# Step 4: Query similarity search\n",
    "query = \"What is the use of embeddings?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "for r in results:\n",
    "    print(r.page_content)\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Use Cases\n",
    "\n",
    "| Use Case        | Description                                                    |\n",
    "| --------------- | -------------------------------------------------------------- |\n",
    "| Semantic Search | Find semantically similar documents instead of keyword matches |\n",
    "| Document Q\\&A   | Retrieve context for RAG pipelines                             |\n",
    "| Clustering      | Group similar texts/documents automatically                    |\n",
    "| Recommendation  | Recommend similar articles, products, or feedback              |\n",
    "| Deduplication   | Detect near-duplicate content                                  |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Architecture / Diagram\n",
    "\n",
    "```python\n",
    "from IPython.display import Image\n",
    "# OpenAI embedding flow\n",
    "Image(\"https://raw.githubusercontent.com/hwchase17/langchain-docs/master/docs/_static/openai_embedding_workflow.png\")\n",
    "```\n",
    "\n",
    "**Description:**\n",
    "\n",
    "* Input text → OpenAI Embedding model → Dense vector representation → Stored in vector DB → Retrieved during query with similarity search → Passed into LLM.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Interview Q\\&A\n",
    "\n",
    "| Question                                         | Answer                                                                                                  |\n",
    "| ------------------------------------------------ | ------------------------------------------------------------------------------------------------------- |\n",
    "| What are embeddings?                             | Vector representations of text that capture semantic meaning.                                           |\n",
    "| Which OpenAI models are used for embeddings?     | `text-embedding-3-small` (fast, cheap, 1536-dim) and `text-embedding-3-large` (high-quality, 3072-dim). |\n",
    "| How are embeddings used in RAG pipelines?        | They are stored in vector DBs and retrieved by similarity search to provide context to LLMs.            |\n",
    "| Why cosine similarity is preferred?              | It normalizes vectors and measures angular closeness, making it scale-independent.                      |\n",
    "| What is the dimensionality of OpenAI embeddings? | 1536 (small model) or 3072 (large model).                                                               |\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095a400",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
